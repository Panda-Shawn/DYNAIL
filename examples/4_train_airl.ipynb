{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Agent using Adversarial Inverse Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we first need an expert. \n",
    "Note that we now use a variant of the CartPole environment from the seals package, which has fixed episode durations. Read more about why we do this [here](https://imitation.readthedocs.io/en/latest/guide/variable_horizon.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000762011/pythonFiles/vscode_datascience_helpers/daemon/daemon_python.py\", line 54, in _decorator\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000762011/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 108, in m_exec_module_observable\n",
      "    self._start_notebook(args, cwd, env)\n",
      "  File \"/root/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000762011/pythonFiles/vscode_datascience_helpers/jupyter_daemon.py\", line 162, in _start_notebook\n",
      "    app.launch_new_instance()\n",
      "  File \"/root/anaconda3/lib/python3.9/site-packages/jupyter_core/application.py\", line 264, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/root/anaconda3/lib/python3.9/site-packages/notebook/notebookapp.py\", line 2292, in start\n",
      "    self.exit(1)\n",
      "  File \"/root/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 836, in exit\n",
      "    sys.exit(exit_status)\n",
      "SystemExit: 1\n",
      "\n",
      "[W 2022-03-24 15:22:05.651 LabApp] 'notebook_dir' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "\n",
      "[W 2022-03-24 15:22:05.651 LabApp] 'config_file' was found in both NotebookApp and ServerApp. This is likely a recent change. This config will only be set in NotebookApp. Please check if you should also config these traits in ServerApp for your purpose.\n",
      "\n",
      "[W 2022-03-24 15:22:05.652 LabApp] 'iopub_data_rate_limit' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "\n",
      "[W 2022-03-24 15:22:05.652 LabApp] 'iopub_data_rate_limit' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\n",
      "\n",
      "[I 2022-03-24 15:22:05.664 LabApp] JupyterLab extension loaded from /root/anaconda3/lib/python3.9/site-packages/jupyterlab\n",
      "\n",
      "[I 2022-03-24 15:22:05.664 LabApp] JupyterLab application directory is /root/anaconda3/share/jupyter/lab\n",
      "\n",
      "[C 15:22:05.667 NotebookApp] Running as root is not recommended. Use --allow-root to bypass.\n",
      "\n",
      "Failed to run jupyter as observable with args notebook --no-browser --notebook-dir=\"/root\" --KernelManager.autorestart=False --config=/tmp/cff56b15-f7c4-42fd-9b16-4c5333034c31/jupyter_notebook_config.py --NotebookApp.iopub_data_rate_limit=10000000000.0. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "import gym\n",
    "import seals\n",
    "\n",
    "env = gym.make(\"Ant-v2\")\n",
    "expert = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=env,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "expert.learn(1000)  # Note: set to 100000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate some expert trajectories, that the discriminator needs to distinguish from the learner's trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "rollouts = rollout.rollout(\n",
    "    expert,\n",
    "    DummyVecEnv([lambda: RolloutInfoWrapper(gym.make(\"Ant-v2\"))] * 5),\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=60),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to set up our AIRL trainer.\n",
    "Note, that the `reward_net` is actually the network of the discriminator.\n",
    "We evaluate the learner before and after training so we can see if it made any progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "import gym\n",
    "import seals\n",
    "\n",
    "\n",
    "venv = DummyVecEnv([lambda: gym.make(\"Ant-v2\")] * 8)\n",
    "learner = PPO(\n",
    "    env=venv,\n",
    "    policy=MlpPolicy,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    ")\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "airl_trainer = AIRL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")\n",
    "\n",
    "learner_rewards_before_training, _ = evaluate_policy(\n",
    "    learner, venv, 100, return_episode_rewards=True\n",
    ")\n",
    "airl_trainer.train(20000)  # Note: set to 300000 for better results\n",
    "learner_rewards_after_training, _ = evaluate_policy(\n",
    "    learner, venv, 100, return_episode_rewards=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the histograms of rewards before and after learning, we can see that the learner is not perfect yet, but it made some progress at least.\n",
    "If not, just re-run the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(np.mean(learner_rewards_after_training))\n",
    "print(np.mean(learner_rewards_before_training))\n",
    "\n",
    "plt.hist(\n",
    "    [learner_rewards_before_training, learner_rewards_after_training],\n",
    "    label=[\"untrained\", \"trained\"],\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
